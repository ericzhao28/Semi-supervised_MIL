{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, clone_model, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, TimeDistributed, Conv2D, Reshape\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Concatenate, CuDNNLSTM, Cropping1D\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "tf.reset_default_graph()\n",
    "TD = TimeDistributed\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "task_learning_rate = 0.005\n",
    "T_in = 16\n",
    "T_total = 100\n",
    "IMG_H = 125\n",
    "IMG_W = 125\n",
    "IMG_CH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit initial conv:  (?, 16, 3, 3, 128)\n"
     ]
    }
   ],
   "source": [
    "def build_autoencoder():\n",
    "  inputs = Input(shape=(T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  # Apply convolutions on the initial image input with increasing channel size.\n",
    "  conv_x = TD(Conv2D(32, kernel_size=(3, 3), strides=(2, 2), activation='relu'))(inputs)\n",
    "  conv_x = TD(MaxPooling2D(pool_size=(2, 2)))(conv_x)\n",
    "  conv_x = TD(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), activation='relu'))(conv_x)\n",
    "  conv_x = TD(MaxPooling2D(pool_size=(2, 2)))(conv_x)\n",
    "  conv_x = TD(Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu'))(conv_x)\n",
    "  print(\"Exit initial conv: \", conv_x.shape)\n",
    "\n",
    "  # We pass the flattened convolution output into a CuDNN-optimized LSTM.\n",
    "  # Outputs are disregarded for training but form the \"encoded\" representation.\n",
    "  enc_x = Reshape((T_in, -1))(conv_x)\n",
    "  encoded = Concatenate()(CuDNNLSTM(1024, return_state=True, return_sequences=False)(enc_x))\n",
    "  return Model(inputs=inputs, outputs=encoded)\n",
    "\n",
    "autoencoder = build_autoencoder()\n",
    "autoencoder.load_weights(\"/home/ubuntu/semisupervised_mil/autoencoder/model_weights_6.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_worse_model():\n",
    "  inputs = Input(shape=(T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  model = TD(Flatten())(inputs)\n",
    "  model = Cropping1D(cropping=(T_in - 1, 0))(model)\n",
    "  model = Reshape((IMG_H, IMG_W, IMG_CH))(model)\n",
    "  model = Convolution2D(32, (3, 3), padding='same', activation=\"relu\")(model)\n",
    "  model = MaxPooling2D(pool_size=(3, 3))(model)\n",
    "  model = Convolution2D(32, (3, 3), padding='same', activation=\"relu\")(model)\n",
    "  model = MaxPooling2D(pool_size=(3, 3))(model)\n",
    "  model = Dropout(0.25)(model)\n",
    "  model = Flatten()(model)\n",
    "  model = Dense(512, activation=\"relu\")(model)\n",
    "  model = Dropout(0.5)(model)\n",
    "  preds = Dense(7, activation=\"tanh\")(model)\n",
    "  \n",
    "  keras_model = Model(inputs=inputs, outputs=preds)\n",
    "  \n",
    "  return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_better_model():\n",
    "  inputs = Input(shape=(T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  model = autoencoder(inputs)\n",
    "  model = Dense(512, activation=\"relu\")(model)\n",
    "  model = Dropout(0.5)(model)\n",
    "  preds = Dense(7, activation=\"tanh\")(model)\n",
    "  keras_model = Model(inputs=inputs, outputs=preds)\n",
    "  \n",
    "  return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO UPDATE\n",
    "better_model = build_better_model()\n",
    "worse_model = build_worse_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model = build_worse_model\n",
    "meta_model = worse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train_input = tf.placeholder(tf.float32, shape=(batch_size, T_in, IMG_H, IMG_W, IMG_CH))\n",
    "train_label = tf.placeholder(tf.float32, shape=(batch_size, 7))\n",
    "# Calulate initial loss\n",
    "before_pred = meta_model(train_input)\n",
    "# Calculate loss and gradient for the task\n",
    "before_loss = tf.reduce_mean(mean_squared_error(train_label, before_pred))\n",
    "before_gradients = tf.gradients(before_loss, meta_model.trainable_weights)\n",
    "# Calculate ethereal weights for task-specific network\n",
    "ethereal = {}\n",
    "for weight, gradient in zip(meta_model.trainable_weights, before_gradients):\n",
    "  ethereal[weight] = weight - task_learning_rate * gradient\n",
    "# Load test data\n",
    "specialized_copy_ops = []\n",
    "test_input = tf.placeholder(tf.float32, shape=(batch_size, T_in, IMG_H, IMG_W, IMG_CH))\n",
    "test_label = tf.placeholder(tf.float32, shape=(batch_size, 7))\n",
    "# Build new ethereal model\n",
    "after_model = build_model()\n",
    "for before_weight, after_weight in zip(meta_model.trainable_weights,\n",
    "                                       after_model.trainable_weights):\n",
    "  specialized_copy_ops.append(tf.assign(after_weight, ethereal[before_weight]))\n",
    "# Calculate the final gradients!\n",
    "after_pred = after_model(test_input)\n",
    "after_loss = tf.reduce_mean(mean_squared_error(test_label, after_pred))\n",
    "after_grads = tf.gradients(after_loss, after_model.trainable_weights)\n",
    "\n",
    "# Update our meta weights!\n",
    "meta_ops = []\n",
    "for before_weight, grad in zip(meta_model.trainable_weights, after_grads):\n",
    "  meta_ops.append(tf.assign(before_weight, before_weight - learning_rate * grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, train_y, test_x, test_y):\n",
    "  sess.run(specialized_copy_ops, feed_dict={train_input: train_x,\n",
    "                                            train_label: train_y})\n",
    "  sess.run(meta_ops, feed_dict={test_input: test_x,\n",
    "                                test_label: test_y})\n",
    "  \n",
    "def test(train_x, train_y, test_x, test_y):\n",
    "  sess.run(specialized_copy_ops, feed_dict={train_input: train_x,\n",
    "                                            train_label: train_y})\n",
    "  return sess.run(after_loss, feed_dict={test_input: test_x,\n",
    "                                         test_label: test_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Train RMSE:  0.016738025\n",
      "Test RMSE:  0.016294705\n",
      "Train RMSE:  0.016334802\n",
      "Test RMSE:  0.015055502\n",
      "Train RMSE:  0.01712546\n",
      "Test RMSE:  0.014283715\n",
      "Train RMSE:  0.013907373\n",
      "Test RMSE:  0.013872366\n",
      "Train RMSE:  0.013950493\n",
      "Test RMSE:  0.01357487\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='results.log',level=logging.DEBUG)\n",
    "\n",
    "\n",
    "# TODO remove 255\n",
    "epochs = 15\n",
    "batches = 23\n",
    "gap = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "  secret_train_x = np.load(\"/home/ubuntu/bigstorage/good/\" + str(batches + 1) + \"_trainx.npy\")/255\n",
    "  secret_train_y = np.load(\"/home/ubuntu/bigstorage/good/\" + str(batches + 1) + \"_trainy.npy\")\n",
    "  secret_test_x = np.load(\"/home/ubuntu/bigstorage/good/\" + str(batches + 1) + \"_testx.npy\")/255\n",
    "  secret_test_y =np.load(\"/home/ubuntu/bigstorage/good/\" + str(batches + 1) + \"_testy.npy\")\n",
    "  \n",
    "  print(\"Epoch: \", i)\n",
    "  logging.info(\"Epoch: \" + str(i))\n",
    "  \n",
    "  for j in range(1, batches):\n",
    "    train_x = np.load(\"/home/ubuntu/bigstorage/good/\" + str(j) + \"_trainx.npy\")/255\n",
    "    train_y = np.load(\"/home/ubuntu/bigstorage/good/\" + str(j) + \"_trainy.npy\")\n",
    "    test_x = np.load(\"/home/ubuntu/bigstorage/good/\" + str(j) + \"_testx.npy\")/255\n",
    "    test_y =np.load(\"/home/ubuntu/bigstorage/good/\" + str(j) + \"_testy.npy\")\n",
    "    \n",
    "    for i in range(0, T_total, gap):\n",
    "      train(train_x[:,i:i+16], train_y[:,i+16], test_x[:,i:i+16], test_y[:,i+16])\n",
    "    \n",
    "    train_score = test(train_x[:,50:50+16], train_y[:,50+16], test_x[:,50:50+16], test_y[:,50+16])\n",
    "    logging.info(\"Train RMSE: \" + str(train_score))\n",
    "    print(\"Train RMSE: \", train_score)\n",
    "\n",
    "    test_score = test(secret_train_x[:,50:50+16], secret_train_y[:,50+16], secret_test_x[:,50:50+16], secret_test_y[:,50+16])\n",
    "    logging.info(\"Test RMSE: \" + str(test_score))\n",
    "    print(\"Test RMSE: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "    \n",
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "x = np.load(\"/home/ubuntu/bigstorage/good/\" + str(24) + \"_trainy.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(x[10][10], dtype=np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
