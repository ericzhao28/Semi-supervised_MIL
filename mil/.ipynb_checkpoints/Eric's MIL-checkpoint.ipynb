{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, clone_model, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, TimeDistributed, Conv2D, Reshape\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Concatenate, CuDNNLSTM\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "# tf.reset_default_graph()\n",
    "TD = TimeDistributed\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "T_in = 16\n",
    "T_total = 100\n",
    "IMG_H = 125\n",
    "IMG_W = 125\n",
    "IMG_CH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder():\n",
    "  inputs = Input(shape=(T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  # Apply convolutions on the initial image input with increasing channel size.\n",
    "  conv_x = TD(Conv2D(32, kernel_size=(3, 3), strides=(2, 2), activation='relu'))(inputs)\n",
    "  conv_x = TD(MaxPooling2D(pool_size=(2, 2)))(conv_x)\n",
    "  conv_x = TD(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), activation='relu'))(conv_x)\n",
    "  conv_x = TD(MaxPooling2D(pool_size=(2, 2)))(conv_x)\n",
    "  conv_x = TD(Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu'))(conv_x)\n",
    "  print(\"Exit initial conv: \", conv_x.shape)\n",
    "\n",
    "  # We pass the flattened convolution output into a CuDNN-optimized LSTM.\n",
    "  # Outputs are disregarded for training but form the \"encoded\" representation.\n",
    "  enc_x = Reshape((T_in, -1))(conv_x)\n",
    "  encoded = Concatenate()(CuDNNLSTM(1024, return_state=True, return_sequences=False)(enc_x))\n",
    "  return Model(inputs=inputs, outputs=encoded)\n",
    "\n",
    "autoencoder = build_autoencoder()\n",
    "autoencoder.load_weights(\"/home/ubuntu/semisupervised_mil/autoencoder/model_weights_6.h5\", by_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_worse_model():\n",
    "  inputs = Input(shape=(T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  model = TD(Convolution2D(32, (3, 3), padding='same', activation=\"relu\"))(inputs)\n",
    "  model = TD(MaxPooling2D(pool_size=(3, 3)))(model)\n",
    "  model = TD(Convolution2D(32, (3, 3), padding='same', activation=\"relu\"))(model)\n",
    "  model = TD(MaxPooling2D(pool_size=(3, 3)))(model)\n",
    "  model = Dropout(0.25)(model)\n",
    "  model = TD(Flatten())(model)\n",
    "  model = CuDNNLSTM(1024, return_state=False, return_sequences=False)(model)\n",
    "  model = Dense(512, activation=\"relu\")(model)\n",
    "  model = Dropout(0.5)(model)\n",
    "  model = Dense(1, activation=\"tanh\")(model)\n",
    "  preds = Dense(7, activation=\"tanh\")(model)\n",
    "  \n",
    "  keras_model = Model(inputs=inputs, outputs=preds)\n",
    "  \n",
    "  return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_better_model():\n",
    "  inputs = Input(shape=(T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  model = autoencoder(inputs)\n",
    "  model = Dropout(0.25)(model)\n",
    "  model = Dense(512, activation=\"relu\")(model)\n",
    "  model = Dropout(0.5)(model)\n",
    "  model = Dense(1, activation=\"tanh\")(model)\n",
    "  preds = Dense(7, activation=\"tanh\")(model)\n",
    "  \n",
    "  keras_model = Model(inputs=inputs, outputs=preds)\n",
    "  \n",
    "  return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO UPDATE\n",
    "build_model = build_better_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train_input = tf.placeholder(tf.float32, shape=(batch_size, T_in, IMG_H, IMG_W, IMG_CH))\n",
    "train_label = tf.placeholder(tf.float32, shape=(batch_size, 7))\n",
    "# Build initial model\n",
    "before_model = build_model()\n",
    "# Calulate initial loss\n",
    "before_pred = before_model(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss and gradient for the task\n",
    "before_loss = tf.reduce_mean(mean_squared_error(train_label, before_pred))\n",
    "before_gradients = tf.gradients(before_loss, before_model.trainable_weights)\n",
    "# Calculate ethereal weights for task-specific network\n",
    "ethereal = {}\n",
    "for weight, gradient in zip(before_model.trainable_weights, before_gradients):\n",
    "  ethereal[weight] = weight - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build specialized\n",
    "specialized_copy_ops = []\n",
    "# Load test data\n",
    "test_input = tf.placeholder(tf.float32, shape=(batch_size, T_in, IMG_H, IMG_W, IMG_CH))\n",
    "test_label = tf.placeholder(tf.float32, shape=(batch_size, 7))\n",
    "# Build new model\n",
    "after_model = build_model()\n",
    "# Assign ethereal values\n",
    "for before_weight, after_weight in zip(before_model.trainable_weights,\n",
    "                                       after_model.trainable_weights):\n",
    "  specialized_copy_ops.append(tf.assign(after_weight, ethereal[before_weight]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new loss\n",
    "after_pred = after_model(test_input)\n",
    "after_loss = tf.reduce_mean(mean_squared_error(test_label, after_pred))\n",
    "# Calculate after gradients\n",
    "after_grads = tf.gradients(after_loss, after_model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_ops = []\n",
    "for before_weight, grad in zip(before_model.trainable_weights, after_grads):\n",
    "  meta_ops.append(tf.assign(before_weight, before_weight - learning_rate * grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, train_y, test_x, test_y):\n",
    "  sess.run(specialized_copy_ops, feed_dict={train_input: train_x,\n",
    "                                            train_label: train_y})\n",
    "  sess.run(meta_ops, feed_dict={test_input: test_x,\n",
    "                                test_label: test_y})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(train_x, train_y, test_x, test_y):\n",
    "  sess.run(specialized_copy_ops, feed_dict={train_input: train_x,\n",
    "                                            train_label: train_y})\n",
    "  return sess.run(after_loss, feed_dict={test_input: test_x,\n",
    "                                         test_label: test_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.random.uniform(size=(batch_size, T_total, IMG_H, IMG_W, IMG_CH))\n",
    "train_y= np.random.uniform(size=(batch_size, T_total, 7))\n",
    "test_x = np.random.uniform(size=(batch_size, T_total, IMG_H, IMG_W, IMG_CH))\n",
    "test_y = np.random.uniform(size=(batch_size, T_total, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD MODEL\n",
    "epochs = 1000\n",
    "for i in range(epochs):\n",
    "  for i in range(15, T_total):\n",
    "    train(train_x[:,i-15:i+1], train_y[:,i], test_x[:,i-15:i+1], test_y[:,i])\n",
    "    print(test(train_x[:,i-15:i+1], train_y[:,i], test_x[:,i-15:i+1], test_y[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD MODEL\n",
    "epochs = 1000\n",
    "for i in range(epochs):\n",
    "  for i in range(15, T_total):\n",
    "    train(train_x[:,i-15:i+1], train_y[:,i], test_x[:,i-15:i+1], test_y[:,i])\n",
    "    print(test(train_x[:,i-15:i+1], train_y[:,i], test_x[:,i-15:i+1], test_y[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "    \n",
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
