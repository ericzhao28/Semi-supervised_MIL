{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Most of this code is taken from\n",
    "the colorama library\n",
    "\"\"\"\n",
    "import logging\n",
    "import sys\n",
    "import traceback as tb\n",
    "\n",
    "COLOR_CODES = {\n",
    "    'utility': 'yellow',\n",
    "    'algorithm': 'lblue',\n",
    "    'traj_opt': 'lgreen',\n",
    "    'dynamics': 'lblue',\n",
    "    'agent': 'lyellow',\n",
    "    'default': 'white',\n",
    "}\n",
    "\n",
    "\n",
    "CSI = '\\033['\n",
    "OSC = '\\033]'\n",
    "BEL = '\\007'\n",
    "\n",
    "\n",
    "def code_to_chars(code):\n",
    "    return CSI + str(code) + 'm'\n",
    "\n",
    "\n",
    "def set_title(title):\n",
    "    return OSC + '2;' + title + BEL\n",
    "\n",
    "\n",
    "def clear_screen(mode=2):\n",
    "    return CSI + str(mode) + 'J'\n",
    "\n",
    "\n",
    "def clear_line(mode=2):\n",
    "    return CSI + str(mode) + 'K'\n",
    "\n",
    "\n",
    "class AnsiCodes(object):\n",
    "    def __init__(self):\n",
    "        # the subclasses declare class attributes which are numbers.\n",
    "        # Upon instantiation we define instance attributes, which are the same\n",
    "        # as the class attributes but wrapped with the ANSI escape sequence\n",
    "        for name in dir(self):\n",
    "            if not name.startswith('_'):\n",
    "                value = getattr(self, name)\n",
    "                setattr(self, name, code_to_chars(value))\n",
    "\n",
    "\n",
    "class AnsiCursor(object):\n",
    "    def UP(self, n=1):\n",
    "        return CSI + str(n) + 'A'\n",
    "\n",
    "    def DOWN(self, n=1):\n",
    "        return CSI + str(n) + 'B'\n",
    "\n",
    "    def FORWARD(self, n=1):\n",
    "        return CSI + str(n) + 'C'\n",
    "\n",
    "    def BACK(self, n=1):\n",
    "        return CSI + str(n) + 'D'\n",
    "\n",
    "    def POS(self, x=1, y=1):\n",
    "        return CSI + str(y) + ';' + str(x) + 'H'\n",
    "\n",
    "\n",
    "class AnsiFore(AnsiCodes):\n",
    "    BLACK = 30\n",
    "    RED = 31\n",
    "    GREEN = 32\n",
    "    YELLOW = 33\n",
    "    BLUE = 34\n",
    "    MAGENTA = 35\n",
    "    CYAN = 36\n",
    "    WHITE = 37\n",
    "    RESET = 39\n",
    "\n",
    "    # These are fairly well supported, but not part of the standard.\n",
    "    LIGHTBLACK_EX = 90\n",
    "    LIGHTRED_EX = 91\n",
    "    LIGHTGREEN_EX = 92\n",
    "    LIGHTYELLOW_EX = 93\n",
    "    LIGHTBLUE_EX = 94\n",
    "    LIGHTMAGENTA_EX = 95\n",
    "    LIGHTCYAN_EX = 96\n",
    "    LIGHTWHITE_EX = 97\n",
    "\n",
    "\n",
    "class AnsiBack(AnsiCodes):\n",
    "    BLACK = 40\n",
    "    RED = 41\n",
    "    GREEN = 42\n",
    "    YELLOW = 43\n",
    "    BLUE = 44\n",
    "    MAGENTA = 45\n",
    "    CYAN = 46\n",
    "    WHITE = 47\n",
    "    RESET = 49\n",
    "\n",
    "    # These are fairly well supported, but not part of the standard.\n",
    "    LIGHTBLACK_EX = 100\n",
    "    LIGHTRED_EX = 101\n",
    "    LIGHTGREEN_EX = 102\n",
    "    LIGHTYELLOW_EX = 103\n",
    "    LIGHTBLUE_EX = 104\n",
    "    LIGHTMAGENTA_EX = 105\n",
    "    LIGHTCYAN_EX = 106\n",
    "    LIGHTWHITE_EX = 107\n",
    "\n",
    "\n",
    "class AnsiStyle(AnsiCodes):\n",
    "    BRIGHT = 1\n",
    "    DIM = 2\n",
    "    NORMAL = 22\n",
    "    RESET_ALL = 0\n",
    "\n",
    "\n",
    "Fore = AnsiFore()\n",
    "Back = AnsiBack()\n",
    "Style = AnsiStyle()\n",
    "Cursor = AnsiCursor()\n",
    "\n",
    "_COLOR_MAP = {\n",
    "    'red': (Fore.RED, Back.RED),\n",
    "    'blue': (Fore.BLUE, Back.BLUE),\n",
    "    'green': (Fore.GREEN, Back.GREEN),\n",
    "    'white': (Fore.WHITE, Back.WHITE),\n",
    "    'black': (Fore.BLACK, Back.BLACK),\n",
    "    'yellow': (Fore.YELLOW, Back.YELLOW),\n",
    "    'magenta': (Fore.MAGENTA, Back.MAGENTA),\n",
    "    'cyan': (Fore.CYAN, Back.CYAN),\n",
    "    'gray': (Fore.LIGHTBLACK_EX, Back.LIGHTBLACK_EX),\n",
    "    'reset': (Fore.RESET, Back.RESET),\n",
    "    'lred': (Fore.LIGHTRED_EX, Back.LIGHTRED_EX),\n",
    "    'lblue': (Fore.LIGHTBLUE_EX, Back.LIGHTBLUE_EX),\n",
    "    'lgreen': (Fore.LIGHTGREEN_EX, Back.LIGHTGREEN_EX),\n",
    "    'lwhite': (Fore.LIGHTWHITE_EX, Back.LIGHTWHITE_EX),\n",
    "    'lblack': (Fore.LIGHTBLACK_EX, Back.LIGHTBLACK_EX),\n",
    "    'lyellow': (Fore.LIGHTYELLOW_EX, Back.LIGHTYELLOW_EX),\n",
    "    'lmagenta': (Fore.LIGHTMAGENTA_EX, Back.LIGHTMAGENTA_EX),\n",
    "    'lcyan': (Fore.LIGHTCYAN_EX, Back.LIGHTCYAN_EX),\n",
    "    None: (Fore.RESET, Back.RESET)\n",
    "}\n",
    "\n",
    "\n",
    "def print_color(text, fore=None, back=None, reset=True, outstream=sys.stdout):\n",
    "    \"\"\"\n",
    "    Prints text in the specified colors\n",
    "\n",
    "    Color codes:\n",
    "        red, blue, green, white, black, yellow, magenta,\n",
    "        cyan, reset\n",
    "\n",
    "    :param text: A string to print\n",
    "    :param fore: A string color code for the foreground.\n",
    "    :param back: A string color code for the background.\n",
    "    :param reset: (Default True) Whether to restore colors back to defaults\n",
    "        after printing.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    reset_ = Fore.RESET + Back.RESET if reset else ''\n",
    "    outstream.write(_COLOR_MAP[fore][0] + _COLOR_MAP[back][1] + text + reset_)\n",
    "\n",
    "\n",
    "def cursorup():\n",
    "    print Cursor.UP()\n",
    "\n",
    "\n",
    "def cursorl():\n",
    "    print Cursor.BACK()\n",
    "\n",
    "\n",
    "def cursorr():\n",
    "    print Cursor.FORWARD()\n",
    "\n",
    "\n",
    "def get_color_code(fname):\n",
    "    for code_dir in COLOR_CODES:\n",
    "        if code_dir in fname:\n",
    "            return COLOR_CODES[code_dir]\n",
    "    return COLOR_CODES['default']\n",
    "\n",
    "\n",
    "def color_string(msg, color=None):\n",
    "    if color==None:\n",
    "        fname, lineno, method, _ = tb.extract_stack()[-2]  # Get caller\n",
    "        color = get_color_code(fname)\n",
    "    return _COLOR_MAP[color][0] + msg + Fore.RESET\n",
    "\n",
    "\n",
    "class ColorLogger(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.logger = logging.getLogger(name)\n",
    "\n",
    "    def info(self, msg, *frmat):\n",
    "        msg = color_string(msg % frmat, color=get_color_code(self.name))\n",
    "        self.logger.info(msg)\n",
    "\n",
    "    def debug(self, msg, *frmat):\n",
    "        msg = color_string(msg % frmat, color=get_color_code(self.name))\n",
    "        self.logger.debug(msg)\n",
    "\n",
    "    def warning(self, msg, *frmat):\n",
    "        msg = color_string(msg % frmat, color='red')\n",
    "        self.logger.warning(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" General utiliy functions \"\"\"\n",
    "import logging\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "import gzip\n",
    "import contextlib\n",
    "import numpy as np\n",
    "import scipy.ndimage as sp_ndimage\n",
    "import os\n",
    "import errno\n",
    "import time\n",
    "import traceback as tb\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def open_zip(filename, mode='r'):\n",
    "    \"\"\"\n",
    "    Open a file; if filename ends with .gz, opens as a gzip file\n",
    "    \"\"\"\n",
    "    if filename.endswith('.gz'):\n",
    "        openfn = gzip.open\n",
    "    else:\n",
    "        openfn = open\n",
    "    yield openfn(filename, mode)\n",
    "\n",
    "class DataLogger(object):\n",
    "    \"\"\"\n",
    "    This class pickles data into files and unpickles data from files.\n",
    "    TODO: Handle logging text to terminal, GUI text, and/or log file at\n",
    "        DEBUG, INFO, WARN, ERROR, FATAL levels.\n",
    "    TODO: Handle logging data to terminal, GUI text/plots, and/or data\n",
    "          files.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def pickle(self, filename, data):\n",
    "        \"\"\" Pickle data into file specified by filename. \"\"\"\n",
    "        with open_zip(filename, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def unpickle(self, filename):\n",
    "        \"\"\" Unpickle data from file specified by filename. \"\"\"\n",
    "        try:\n",
    "            with open_zip(filename, 'rb') as f:\n",
    "                result = pickle.load(f)\n",
    "            return result\n",
    "        except IOError:\n",
    "            LOGGER.debug('Unpickle error. Cannot find file: %s', filename)\n",
    "            return None\n",
    "            \n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def extract_demo_dict(demo_file):\n",
    "    if type(demo_file) is not list:\n",
    "        demos = DataLogger().unpickle(demo_file)\n",
    "    else:\n",
    "        demos = {}\n",
    "        for i in xrange(0, len(demo_file)):\n",
    "            with Timer('Extracting demo file %d' % i):\n",
    "                demos[i] = DataLogger().unpickle(demo_file[i])\n",
    "    return demos\n",
    "\n",
    "class Timer(object):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        new_time = time.time() - self.time_start\n",
    "        fname, lineno, method, _ = tb.extract_stack()[-2]  # Get caller\n",
    "        _, fname = os.path.split(fname)\n",
    "        id_str = '%s:%s' % (fname, method)\n",
    "        print 'TIMER:'+color_string('%s: %s (Elapsed: %fs)' % (id_str, self.message, new_time), color='gray')\n",
    "\n",
    "def load_scale_and_bias(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        scale = data['scale']\n",
    "        bias = data['bias']\n",
    "    return scale, bias\n",
    "    \n",
    "def generate_noise(T, dU):\n",
    "    \"\"\"\n",
    "    Generate a T x dU gaussian-distributed noise vector. This will\n",
    "    approximately have mean 0 and variance 1, ignoring smoothing.\n",
    "\n",
    "    Args:\n",
    "        T: Number of time steps.\n",
    "        dU: Dimensionality of actions.\n",
    "    Hyperparams:\n",
    "        smooth: Whether or not to perform smoothing of noise.\n",
    "        var : If smooth=True, applies a Gaussian filter with this\n",
    "            variance.\n",
    "        renorm : If smooth=True, renormalizes data to have variance 1\n",
    "            after smoothing.\n",
    "    \"\"\"\n",
    "    var = 2.0\n",
    "    noise = np.random.randn(T, dU)\n",
    "    # Smooth noise. This violates the controller assumption, but\n",
    "    # might produce smoother motions.\n",
    "    for i in range(dU):\n",
    "        noise[:, i] = sp_ndimage.filters.gaussian_filter(noise[:, i], var)\n",
    "    variance = np.var(noise, axis=0)\n",
    "    noise = noise / np.sqrt(variance)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Utility functions for tensorflow. \"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "import numpy as np\n",
    "\n",
    "def safe_get(name, *args, **kwargs):\n",
    "    \"\"\" Same as tf.get_variable, except flips on reuse_variables automatically \"\"\"\n",
    "    try:\n",
    "        return tf.get_variable(name, *args, **kwargs)\n",
    "    except ValueError:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        return tf.get_variable(name, *args, **kwargs)\n",
    "\n",
    "def init_weights(shape, name=None):\n",
    "    shape = tuple(shape)\n",
    "    weights = np.random.normal(scale=0.01, size=shape).astype('f')\n",
    "    return safe_get(name, list(shape), initializer=tf.constant_initializer(weights), dtype=tf.float32)\n",
    "    \n",
    "def init_bias(shape, name=None):\n",
    "    return safe_get(name, initializer=tf.zeros(shape, dtype=tf.float32))\n",
    "\n",
    "def init_fc_weights_xavier(shape, name=None):\n",
    "    fc_initializer =  tf.contrib.layers.xavier_initializer(dtype=tf.float32)\n",
    "    return safe_get(name, list(shape), initializer=fc_initializer, dtype=tf.float32)\n",
    "\n",
    "def init_conv_weights_xavier(shape, name=None):\n",
    "    conv_initializer =  tf.contrib.layers.xavier_initializer_conv2d(dtype=tf.float32)\n",
    "    return safe_get(name, list(shape), initializer=conv_initializer, dtype=tf.float32)\n",
    "    \n",
    "def init_fc_weights_snn(shape, name=None):\n",
    "    weights = np.random.normal(scale=np.sqrt(1.0/shape[0]), size=shape).astype('f')\n",
    "    return safe_get(name, list(shape), initializer=tf.constant_initializer(weights), dtype=tf.float32)\n",
    "\n",
    "def init_conv_weights_snn(shape, name=None):\n",
    "    weights = np.random.normal(scale=np.sqrt(1.0/(shape[0]*shape[1]*shape[2])), size=shape).astype('f')\n",
    "    return safe_get(name, list(shape), initializer=tf.constant_initializer(weights), dtype=tf.float32)\n",
    "\n",
    "def batched_matrix_vector_multiply(vector, matrix):\n",
    "    \"\"\" computes x^T A in mini-batches. \"\"\"\n",
    "    vector_batch_as_matricies = tf.expand_dims(vector, [1])\n",
    "    mult_result = tf.matmul(vector_batch_as_matricies, matrix)\n",
    "    squeezed_result = tf.squeeze(mult_result, [1])\n",
    "    return squeezed_result\n",
    "\n",
    "def euclidean_loss_layer(a, b, multiplier=100.0, use_l1=False, eps=0.01):\n",
    "    \"\"\" Math:  out = (action - mlp_out)'*precision*(action-mlp_out)\n",
    "                    = (u-uhat)'*A*(u-uhat)\"\"\"\n",
    "    multiplier = tf.constant(multiplier, dtype='float') #for bc #10000\n",
    "    uP =a*multiplier-b*multiplier\n",
    "    if use_l1:\n",
    "        return tf.reduce_mean(eps*tf.square(uP) + tf.abs(uP))\n",
    "    return tf.reduce_mean(tf.square(uP))\n",
    "\n",
    "def conv2d(img, w, b, strides=[1, 1, 1, 1], is_dilated=False):\n",
    "    if is_dilated:\n",
    "        layer = tf.nn.atrous_conv2d(img, w, rate=2, padding='SAME') + b\n",
    "    else:\n",
    "        layer = tf.nn.conv2d(img, w, strides=strides, padding='SAME') + b\n",
    "    return layer\n",
    "            \n",
    "def dropout(layer, keep_prob=0.9, is_training=True, name=None, selu=False):\n",
    "    if selu:\n",
    "        return dropout_selu(layer, 1.0 - keep_prob, name=name, training=is_training)\n",
    "    if is_training:\n",
    "        return tf.nn.dropout(layer, keep_prob=keep_prob, name=name)\n",
    "    else:\n",
    "        return tf.add(layer, 0, name=name)\n",
    "\n",
    "def norm(layer, norm_type='batch_norm', decay=0.9, id=0, is_training=True, activation_fn=tf.nn.relu, prefix='conv_'):\n",
    "    if norm_type != 'batch_norm' and norm_type != 'layer_norm':\n",
    "        return tf.nn.relu(layer)\n",
    "    with tf.variable_scope('norm_layer_%s%d' % (prefix, id)) as vs:\n",
    "        if norm_type == 'batch_norm':\n",
    "            if is_training:\n",
    "                try:\n",
    "                    layer = tf.contrib.layers.batch_norm(layer, is_training=True, center=True,\n",
    "                        scale=False, decay=decay, activation_fn=activation_fn, updates_collections=None, scope=vs) # updates_collections=None\n",
    "                except ValueError:\n",
    "                    layer = tf.contrib.layers.batch_norm(layer, is_training=True, center=True,\n",
    "                        scale=False, decay=decay, activation_fn=activation_fn, updates_collections=None, scope=vs, reuse=True) # updates_collections=None\n",
    "            else:\n",
    "                layer = tf.contrib.layers.batch_norm(layer, is_training=False, center=True,\n",
    "                    scale=False, decay=decay, activation_fn=activation_fn, updates_collections=None, scope=vs, reuse=True) # updates_collections=None\n",
    "        elif norm_type == 'layer_norm': # layer_norm\n",
    "            # Take activation_fn out to apply lrelu\n",
    "            try:\n",
    "                layer = activation_fn(tf.contrib.layers.layer_norm(layer, center=True,\n",
    "                    scale=False, scope=vs)) # updates_collections=None\n",
    "                \n",
    "            except ValueError:\n",
    "                layer = activation_fn(tf.contrib.layers.layer_norm(layer, center=True,\n",
    "                    scale=False, scope=vs, reuse=True))\n",
    "        elif norm_type == 'selu':\n",
    "            layer = selu(layer)\n",
    "        else:\n",
    "            raise NotImplementedError('Other types of norm not implemented.')\n",
    "        return layer\n",
    "        \n",
    "class VBN(object):\n",
    "    \"\"\"\n",
    "    Virtual Batch Normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, name, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        x is the reference batch\n",
    "        \"\"\"\n",
    "        assert isinstance(epsilon, float)\n",
    "\n",
    "        shape = x.get_shape().as_list()\n",
    "        with tf.variable_scope(name) as scope:\n",
    "            self.epsilon = epsilon\n",
    "            self.name = name\n",
    "            self.mean = tf.reduce_mean(x, [0, 1, 2], keep_dims=True)\n",
    "            self.mean_sq = tf.reduce_mean(tf.square(x), [0, 1, 2], keep_dims=True)\n",
    "            self.batch_size = int(x.get_shape()[0])\n",
    "            assert x is not None\n",
    "            assert self.mean is not None\n",
    "            assert self.mean_sq is not None\n",
    "            out = tf.nn.relu(self._normalize(x, self.mean, self.mean_sq, \"reference\"))\n",
    "            self.reference_output = out\n",
    "\n",
    "    def __call__(self, x, update=False):\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "            if not update:\n",
    "                new_coeff = 1. / (self.batch_size + 1.)\n",
    "                old_coeff = 1. - new_coeff\n",
    "                new_mean = tf.reduce_mean(x, [1, 2], keep_dims=True)\n",
    "                new_mean_sq = tf.reduce_mean(tf.square(x), [1, 2], keep_dims=True)\n",
    "                mean = new_coeff * new_mean + old_coeff * self.mean\n",
    "                mean_sq = new_coeff * new_mean_sq + old_coeff * self.mean_sq\n",
    "                out = tf.nn.relu(self._normalize(x, mean, mean_sq, \"live\"))\n",
    "            # Update the mean and mean_sq when passing the reference data\n",
    "            else:\n",
    "                self.mean = tf.reduce_mean(x, [0, 1, 2], keep_dims=True)\n",
    "                self.mean_sq = tf.reduce_mean(tf.square(x), [0, 1, 2], keep_dims=True)\n",
    "                out = tf.nn.relu(self._normalize(x, self.mean, self.mean_sq, \"reference\"))\n",
    "            return out\n",
    "\n",
    "    def _normalize(self, x, mean, mean_sq, message):\n",
    "        # make sure this is called with a variable scope\n",
    "        shape = x.get_shape().as_list()\n",
    "        assert len(shape) == 4\n",
    "        self.gamma = safe_get(\"gamma\", [shape[-1]],\n",
    "                                initializer=tf.random_normal_initializer(1., 0.02))\n",
    "        gamma = tf.reshape(self.gamma, [1, 1, 1, -1])\n",
    "        self.beta = safe_get(\"beta\", [shape[-1]],\n",
    "                                initializer=tf.constant_initializer(0.))\n",
    "        beta = tf.reshape(self.beta, [1, 1, 1, -1])\n",
    "        assert self.epsilon is not None\n",
    "        assert mean_sq is not None\n",
    "        assert mean is not None\n",
    "        std = tf.sqrt(self.epsilon + mean_sq - tf.square(mean))\n",
    "        out = x - mean\n",
    "        out = out / std\n",
    "        out = out * gamma\n",
    "        out = out + beta\n",
    "        return out\n",
    "\n",
    "def max_pool(img, k):\n",
    "    return tf.nn.max_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# Consider stride size when using xavier for fp network\n",
    "def get_xavier_weights(filter_shape, poolsize=(2, 2), name=None):\n",
    "    fan_in = np.prod(filter_shape[1:])\n",
    "    fan_out = (filter_shape[0] * np.prod(filter_shape[2:]) //\n",
    "               np.prod(poolsize))\n",
    "\n",
    "    low = -4*np.sqrt(6.0/(fan_in + fan_out)) # use 4 for sigmoid, 1 for tanh activation\n",
    "    high = 4*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    weights = np.random.uniform(low=low, high=high, size=filter_shape)\n",
    "    return safe_get(name, filter_shape, initializer=tf.constant_initializer(weights))\n",
    "\n",
    "def get_he_weights(filter_shape, name=None):\n",
    "    fan_in = np.prod(filter_shape[1:])\n",
    "\n",
    "    stddev = np.sqrt(2.6/fan_in)\n",
    "    weights = stddev * np.random.randn(filter_shape[0], filter_shape[1], filter_shape[2], filter_shape[3])\n",
    "    return safe_get(name, filter_shape, initializer=tf.constant_initializer(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class a():\n",
    "  pass\n",
    "\n",
    "FLAGS = a()\n",
    "FLAGS.num_updates = 1\n",
    "FLAGS.update_batch_size = 1\n",
    "FLAGS.meta_batch_size = 12\n",
    "FLAGS.meta_lr = 0.001\n",
    "FLAGS.T = 50\n",
    "FLAGS.num_filters = 64\n",
    "FLAGS.num_strides = 3\n",
    "FLAGS.num_conv_layers = 5\n",
    "FLAGS.filter_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "a instance has no attribute 'im_width'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-64b9c12e7993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m'strides'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_strides\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_conv_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_strides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m'filter_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;34m'image_width'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m'image_height'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m'image_channels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: a instance has no attribute 'im_width'"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "graph = tf.Graph()\n",
    "sess = tf.Session(graph=graph)\n",
    "network_config = {\n",
    "    'num_filters': [FLAGS.num_filters]*FLAGS.num_conv_layers,\n",
    "    'strides': [[1, 2, 2, 1]]*FLAGS.num_strides + [[1, 1, 1, 1]]*(FLAGS.num_conv_layers-FLAGS.num_strides),\n",
    "    'filter_size': FLAGS.filter_size,\n",
    "    'image_width': FLAGS.im_width,\n",
    "    'image_height': FLAGS.im_height,\n",
    "    'image_channels': FLAGS.num_channels,\n",
    "    'n_layers': FLAGS.num_fc_layers,\n",
    "    'layer_size': FLAGS.layer_size,\n",
    "    'initialization': FLAGS.init,\n",
    "}\n",
    "data_generator = DataGenerator()\n",
    "state_idx = data_generator.state_idx\n",
    "img_idx = range(len(state_idx), len(state_idx)+FLAGS.im_height*FLAGS.im_width*FLAGS.num_channels)\n",
    "# need to compute x_idx and img_idx from data_generator\n",
    "model = MIL(data_generator._dU, state_idx=state_idx, img_idx=img_idx, network_config=network_config)\n",
    "\n",
    "data_generator.generate_batches(noisy=FLAGS.use_noisy_demos)\n",
    "with graph.as_default():\n",
    "    train_image_tensors = data_generator.make_batch_tensor(network_config, restore_iter=FLAGS.restore_iter)\n",
    "    inputa = train_image_tensors[:, :FLAGS.update_batch_size*FLAGS.T, :]\n",
    "    inputb = train_image_tensors[:, FLAGS.update_batch_size*FLAGS.T:, :]\n",
    "    train_input_tensors = {'inputa': inputa, 'inputb': inputb}\n",
    "    val_image_tensors = data_generator.make_batch_tensor(network_config, restore_iter=FLAGS.restore_iter, train=False)\n",
    "    inputa = val_image_tensors[:, :FLAGS.update_batch_size*FLAGS.T, :]\n",
    "    inputb = val_image_tensors[:, FLAGS.update_batch_size*FLAGS.T:, :]\n",
    "    val_input_tensors = {'inputa': inputa, 'inputb': inputb}\n",
    "model.init_network(graph, input_tensors=train_input_tensors, restore_iter=FLAGS.restore_iter)\n",
    "model.init_network(graph, input_tensors=val_input_tensors, restore_iter=FLAGS.restore_iter, prefix='Validation_')\n",
    "\n",
    "with graph.as_default():\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    train(graph, model, saver, sess, data_generator, log_dir, restore_itr=FLAGS.restore_iter)\n",
    "    \n",
    "    for itr in range(TOTAL_ITERS):\n",
    "        state, tgt_mu = data_generator.generate_data_batch(itr)\n",
    "        statea = state[:, :FLAGS.update_batch_size*FLAGS.T, :]\n",
    "        stateb = state[:, FLAGS.update_batch_size*FLAGS.T:, :]\n",
    "        actiona = tgt_mu[:, :FLAGS.update_batch_size*FLAGS.T, :]\n",
    "        actionb = tgt_mu[:, FLAGS.update_batch_size*FLAGS.T:, :]\n",
    "        feed_dict = {model.statea: statea,\n",
    "                    model.stateb: stateb,\n",
    "                    model.actiona: actiona,\n",
    "                    model.actionb: actionb}\n",
    "\n",
    "        with graph.as_default():\n",
    "            results = sess.run([model.train_op], feed_dict=feed_dict)\n",
    "\n",
    "        if itr != 0 and itr % TEST_PRINT_INTERVAL == 0:\n",
    "            if FLAGS.val_set_size > 0:\n",
    "                val_state, val_act = data_generator.generate_data_batch(itr, train=False)\n",
    "                statea = val_state[:, :FLAGS.update_batch_size*FLAGS.T, :]\n",
    "                stateb = val_state[:, FLAGS.update_batch_size*FLAGS.T:, :]\n",
    "                actiona = val_act[:, :FLAGS.update_batch_size*FLAGS.T, :]\n",
    "                actionb = val_act[:, FLAGS.update_batch_size*FLAGS.T:, :]\n",
    "                feed_dict = {model.statea: statea,\n",
    "                            model.stateb: stateb,\n",
    "                            model.actiona: actiona,\n",
    "                            model.actionb: actionb}\n",
    "                with graph.as_default():\n",
    "                    results = sess.run([model.val_summ_op, model.val_total_loss1,\n",
    "                                        model.val_total_losses2[model.num_updates-1]],\n",
    "                                       feed_dict=feed_dict)\n",
    "                print 'Test results: average preloss is %.2f, average postloss is %.2f' % (np.mean(results[1]), np.mean(results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('num_filters', 64, 'number of filters for conv nets -- 64 for placing, 16 for pushing, 40 for reaching.')\n",
    "flags.DEFINE_integer('filter_size', 3, 'filter size for conv nets -- 3 for placing, 5 for pushing, 3 for reaching.')\n",
    "flags.DEFINE_integer('num_conv_layers', 5, 'number of conv layers -- 5 for placing, 4 for pushing, 3 for reaching.')\n",
    "flags.DEFINE_integer('num_strides', 3, 'number of conv layers with strided filters -- 3 for placing, 4 for pushing, 3 for"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
