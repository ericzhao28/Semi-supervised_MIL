{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, clone_model, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, TimeDistributed, Conv2D, Reshape\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Concatenate, CuDNNLSTM, Cropping1D\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "# tf.reset_default_graph()\n",
    "TD = TimeDistributed\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "T_in = 16\n",
    "T_total = 100\n",
    "IMG_H = 125\n",
    "IMG_W = 125\n",
    "IMG_CH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit initial conv:  (?, 16, 3, 3, 128)\n"
     ]
    }
   ],
   "source": [
    "def build_autoencoder():\n",
    "  inputs = Input(shape=(T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  # Apply convolutions on the initial image input with increasing channel size.\n",
    "  conv_x = TD(Conv2D(32, kernel_size=(3, 3), strides=(2, 2), activation='relu'))(inputs)\n",
    "  conv_x = TD(MaxPooling2D(pool_size=(2, 2)))(conv_x)\n",
    "  conv_x = TD(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), activation='relu'))(conv_x)\n",
    "  conv_x = TD(MaxPooling2D(pool_size=(2, 2)))(conv_x)\n",
    "  conv_x = TD(Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu'))(conv_x)\n",
    "  print(\"Exit initial conv: \", conv_x.shape)\n",
    "\n",
    "  # We pass the flattened convolution output into a CuDNN-optimized LSTM.\n",
    "  # Outputs are disregarded for training but form the \"encoded\" representation.\n",
    "  enc_x = Reshape((T_in, -1))(conv_x)\n",
    "  encoded = Concatenate()(CuDNNLSTM(1024, return_state=True, return_sequences=False)(enc_x))\n",
    "  return Model(inputs=inputs, outputs=encoded)\n",
    "\n",
    "autoencoder = build_autoencoder()\n",
    "autoencoder.load_weights(\"/home/ubuntu/semisupervised_mil/autoencoder/model_weights_6.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_worse_model():\n",
    "  inputs = Input(shape=(T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  model = TD(Flatten())(inputs)\n",
    "  model = Cropping1D(cropping=(T_in - 1, 0))(model)\n",
    "  model = Reshape((IMG_H, IMG_W, IMG_CH))(model)\n",
    "  model = Convolution2D(32, (3, 3), padding='same', activation=\"relu\")(model)\n",
    "  model = MaxPooling2D(pool_size=(3, 3))(model)\n",
    "  model = Convolution2D(32, (3, 3), padding='same', activation=\"relu\")(model)\n",
    "  model = MaxPooling2D(pool_size=(3, 3))(model)\n",
    "  model = Dropout(0.25)(model)\n",
    "  model = Flatten()(model)\n",
    "  model = Dense(512, activation=\"relu\")(model)\n",
    "  model = Dropout(0.5)(model)\n",
    "  preds = Dense(7, activation=\"tanh\")(model)\n",
    "  \n",
    "  keras_model = Model(inputs=inputs, outputs=preds)\n",
    "  \n",
    "  return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_better_model():\n",
    "  inputs = Input(shape=(T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  model = autoencoder(inputs)\n",
    "  model = Dense(512, activation=\"relu\")(model)\n",
    "  model = Dropout(0.5)(model)\n",
    "  preds = Dense(7, activation=\"tanh\")(model)\n",
    "  keras_model = Model(inputs=inputs, outputs=preds)\n",
    "  \n",
    "  return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(meta_model, build_model):\n",
    "  # Load train data\n",
    "  train_X = tf.placeholder(tf.float32, shape=(batch_size, T_total, T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  train_Y = tf.placeholder(tf.float32, shape=(batch_size, T_total, 7))\n",
    "  X_ = tf.reshape(train_X, shape=(batch_size * T_total, T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  Y_ = tf.reshape(train_Y, shape=(batch_size * T_total, 7))\n",
    "\n",
    "  # Ops: [[build_model], [update_model], [update meta]].\n",
    "  ops = [[], [], []]\n",
    "\n",
    "  # Copy meta model to the initial model.\n",
    "  initial_pred = meta_model(X_)\n",
    "  initial_loss = tf.reduce_mean(mean_squared_error(Y_, initial_pred))\n",
    "  initial_grads = tf.gradients(initial_loss, meta_model.trainable_weights)\n",
    "\n",
    "  # Construct ethereal model.\n",
    "  ethereal_model = build_model()\n",
    "  mid_weights = []\n",
    "  for initial_weight, ethereal_weight, initial_gradient in zip(initial_model.trainable_weights,\n",
    "                                                               ethereal_model.trainable_weights,\n",
    "                                                               initial_grads):\n",
    "    mid_weights.append(initial_weight - learning_rate * initial_gradient)\n",
    "    ops[1].append(tf.assign(ethereal_weight, mid_weights[-1]))\n",
    "\n",
    "  # Load test data\n",
    "  test_X = tf.placeholder(tf.float32, shape=(batch_size, T_total, T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  test_Y = tf.placeholder(tf.float32, shape=(batch_size, T_total, 7))\n",
    "  X_ = tf.reshape(test_X, shape=(batch_size * T_total, T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  Y_ = tf.reshape(test_Y, shape=(batch_size * T_total, 7))\n",
    "  \n",
    "  # Calulate final loss.\n",
    "  final_pred = ethereal_model(X_)\n",
    "  final_loss = tf.reduce_mean(mean_squared_error(Y_, final_pred))\n",
    "  partial_grads = tf.gradients(final_loss, ethereal_model.trainable_weights)\n",
    "  \n",
    "  # Accumulate gradients\n",
    "  all_grads = {}\n",
    "  for meta_weight, partial_grad in zip(meta_model.trainable_weights, partial_grads):\n",
    "    all_grads[meta_weight] = [partial_grad]\n",
    "\n",
    "  for meta_weight, partial_grad, initial_grad in zip(meta_model.trainable_weights, partial_grads, initial_grad):\n",
    "\n",
    "  # for mid_weight in mid_weights:\n",
    "  for meta_weight, initial_gradient in zip(meta_model.trainable_weights, initial_grads):\n",
    "    total_gradient = initial_gradient * (- learning_rate)\n",
    "    ops[2].append(tf.assign(meta_weight, meta_weight - learning_rate * total_gradient))\n",
    "\n",
    "  return train_X, train_Y, test_X, test_Y, final_loss, ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(meta_model, build_model):\n",
    "  # Load train data\n",
    "  train_X = tf.placeholder(tf.float32, shape=(batch_size, T_total, T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  train_Y = tf.placeholder(tf.float32, shape=(batch_size, T_total, 7))\n",
    "  X_ = tf.reshape(train_X, shape=(batch_size * T_total, T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  Y_ = tf.reshape(train_Y, shape=(batch_size * T_total, 7))\n",
    "\n",
    "  # Ops: [[build_model], [update_model], [update meta]].\n",
    "  ops = [[], [], []]\n",
    "\n",
    "  # Copy meta model to the initial model.\n",
    "  initial_model = build_model()\n",
    "  for meta_weight, initial_weight in zip(meta_model.trainable_weights, initial_model.trainable_weights):\n",
    "    ops[0].append(tf.assign(initial_weight, meta_weight))\n",
    "  initial_pred = initial_model(X_)\n",
    "  initial_loss = tf.reduce_mean(mean_squared_error(Y_, initial_pred))\n",
    "  initial_grads = tf.gradients(initial_loss, initial_model.trainable_weights)\n",
    "  \n",
    "  # WE ONLY NEED: initial_grads, initial_model\n",
    "\n",
    "  # Construct ethereal model.\n",
    "  ethereal_model = build_model()\n",
    "  mid_weights = []\n",
    "  for initial_weight, ethereal_weight, initial_gradient in zip(initial_model.trainable_weights,\n",
    "                                                               ethereal_model.trainable_weights,\n",
    "                                                               initial_grads):\n",
    "    mid_weights.append(initial_weight - learning_rate * initial_gradient)\n",
    "    ops[1].append(tf.assign(ethereal_weight, mid_weights[-1]))\n",
    "\n",
    "  # Load test data\n",
    "  test_X = tf.placeholder(tf.float32, shape=(batch_size, T_total, T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  test_Y = tf.placeholder(tf.float32, shape=(batch_size, T_total, 7))\n",
    "  X_ = tf.reshape(test_X, shape=(batch_size * T_total, T_in, IMG_H, IMG_W, IMG_CH))\n",
    "  Y_ = tf.reshape(test_Y, shape=(batch_size * T_total, 7))\n",
    "  \n",
    "  # Calulate final loss.\n",
    "  final_pred = ethereal_model(X_)\n",
    "  final_loss = tf.reduce_mean(mean_squared_error(Y_, final_pred))\n",
    "  partial_grads = tf.gradients(final_loss, ethereal_model.trainable_weights)\n",
    "\n",
    "  # for mid_weight in mid_weights:\n",
    "  for meta_weight, mid_weight, initial_gradient in zip(meta_model.trainable_weights, mid_weights, initial_grads):\n",
    "    back_grads = tf.gradients(mid_weight, initial_model.trainable_weights)\n",
    "    for back_gradient in back_grads:\n",
    "      total_gradient = initial_gradient * back_gradient\n",
    "    ops[2].append(tf.assign(meta_weight, meta_weight - learning_rate * total_gradient))\n",
    "\n",
    "  return train_X, train_Y, test_X, test_Y, final_loss, ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO UPDATE\n",
    "better_model = build_better_model()\n",
    "worse_model = build_worse_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Second-order gradient for while loops not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2326\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2327\u001b[0;31m           \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2328\u001b[0m           \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'gradients/model_4/model_1/time_distributed_1/while/convolution/Enter_grad/b_acc_3' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m       \u001b[0mxla_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaCompile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2331\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2332\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operation 'gradients/model_4/model_1/time_distributed_1/while/convolution/Enter_grad/b_acc_3' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6506035f24f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmeta_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbetter_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbuild_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_better_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-c92bf6aa4e96>\u001b[0m in \u001b[0;36minit_model\u001b[0;34m(meta_model, build_model)\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;31m# for mid_weight in mid_weights:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mmeta_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmid_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_gradient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmid_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mback_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mback_gradient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mback_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0mtotal_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_gradient\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mback_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    492\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\n\u001b[0;32m--> 494\u001b[0;31m                             gate_gradients, aggregation_method, stop_gradients)\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 636\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    637\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    383\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaScope\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 636\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    637\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_grad.py\u001b[0m in \u001b[0;36m_ExitGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mop_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Second-order gradient for while loops not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Second-order gradient for while loops not supported."
     ]
    }
   ],
   "source": [
    "meta_model = better_model\n",
    "build_model = build_better_model\n",
    "train_X, train_Y, test_X, test_Y, loss, ops = init_model(meta_model, build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trX, trY, teX, teY):\n",
    "  sess.run(ops[0], feed_dict={train_X: trX,\n",
    "                              train_Y: trY,\n",
    "                              test_X: teX,\n",
    "                              test_Y: teY})\n",
    "  sess.run(ops[1])\n",
    "  sess.run(ops[2])\n",
    "  \n",
    "def test(trX, trY, teX, teY):\n",
    "  sess.run(ops[0], feed_dict={train_X: trX,\n",
    "                              train_Y: trY,\n",
    "                              test_X: teX,\n",
    "                              test_Y: teY})\n",
    "  sess.run(ops[1])\n",
    "  return sess.run(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove 255\n",
    "epochs = 1\n",
    "batches = 2\n",
    "gap = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "  for j in range(1, batches):\n",
    "    train_x = np.load(\"/home/ubuntu/bigstorage/good/\" + str(j) + \"_trainx.npy\")/255\n",
    "    train_y = np.load(\"/home/ubuntu/bigstorage/good/\" + str(j) + \"_trainy.npy\")/255\n",
    "    test_x = np.load(\"/home/ubuntu/bigstorage/good/\" + str(j) + \"_testx.npy\")/255\n",
    "    test_y =np.load(\"/home/ubuntu/bigstorage/good/\" + str(j) + \"_testy.npy\")/255\n",
    "    \n",
    "    for i in range(0, T_total - 16, 10):\n",
    "      print(i)\n",
    "      train(train_x[:,i:i+16], train_y[:,i+16], test_x[:,i:i+16], test_y[:,i+16])\n",
    "      print(test(train_x[:,i:i+16], train_y[:,i+16], test_x[:,i:i+16], test_y[:,i+16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "    \n",
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
